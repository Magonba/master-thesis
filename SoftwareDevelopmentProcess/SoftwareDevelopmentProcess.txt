4. Software Development Process

In this chapter the application development process is described.

The software development process can be partitioned in four parts which are the requirements engineering, the database development, the server-side application development and the client-side application development. While the requirements engineering lays out a plan for the development of the entire application, the other three components follow a bottom-up approach where the database is the piece of software that is furthest away from the user and the client-side application is the closest to the user. The server-side application is then the connection between the database and the client-side application.

In theory, these four components could be concluded one after the other, but in practise they provided more of a general guideline in the development process, where it was possible to adapt previous steps in later stages (e.g. the requirements could be adapted when the server-side development was in progress). The reason that those steps were not concluded in a purely chronological order is that, on one hand, it is difficult to completely grasp the requirements at the beginning of the project (adaptations of requirements may happen in later stages of the process) and, on the other hand, since the database, the server-side and the client-side applications are connected between each other unexpected changes in one of those components might lead to changes in the other two components.

The server-side and client-side development can additionally be splitted in a coding and a testing part. The coding and the testing of the server-side and client-side application was done in an iterative way where first some part of the application was coded and then tested as needed.

The first subchapter is going to describe the requirements engineering of the application. Concretely, it will look at the use cases that were developed and the prototype that was created at the beginning of the process. The second subchapter is going to look more in depth into the database design. The third subchapter will take a look at the architecture of the server-side application and how it interacts with waste collection optimization algorithms developed by Bürgy et al. 20xx. Lastly, the fourth subchapter explains the functioning of the client-side application. In each of the subchapters the used technologies are going to be shortly presented.

4.1. Requirements Engineering

Requirements engineering processes do (among other things) elicit, analyse, negotiate, specify and validate software requirements and are considered one of the most critical steps in the software development process (Tahir & Ahmad, 2010, p.1). It can have a negative impact on later stages of the software development process if it is poorly executed and is crucial in order to finish software projects successfully since it is its foundation (ur Rehman et al., 2013, p.1). This clearly shows the importance of requirements engineering and why this step should not be skipped.

The chosen technique to specify software requirements was a screen mockup. Screen mockups have been shown to be very useful for understanding the requirements of a project (Ricca et al., 20xx)(Ricca et al., 20xx)(Ricca et al., 20xx). Additionally a use case diagram was created. Use case diagrams, even though vague, help to not lose the fundamental functionalities that the application must have out of sight (Head First Object Oriented Analysis and Design, Book, McLaughlin).

The screen mockup was developed iteratively by meeting the stakeholders of the project regularly. The first meeting notes were taken about how the application could look like. Then for the subsequent meeting a screen mockup was developed and presented to the stakeholders which could then input their improvement propositions. Then before each subsequent meeting the mockup was improved based on the inputs of the stakeholders which again could input their improvement propositions. This process was repeated until the mockup was judged to sufficiently correspond to what the stakeholders imagined.

[insert image representing the process]

The developed screen mockup shows, on one hand, generic application functionalities such as log in and sign up functionalities. On the other hand, it shows application specific functionalities such as a scenario based input specification functionality (e.g. the user can input how much garbage is produced on each node and save this specification) and a solution request functionality which allows the user to request individualized solutions for its specific municipality by choosing its input parameters (which are among other things the scenario based input specifications). Additionally, the administrator should be able to create a new project for a municipality by uploading its map to the application, manage project permissions (i.e. who is allowed to access a certain project) and delete users if needed. The developed use case diagram of the application can be seen on graphic xxx.

[show: use case diagram]

After agreeing on the functionalities, the next step was to specify the general architecuture of the project. First of all, the components that are going to make up the application are: the database, the server, the client-side application and the waste collection optimization algorithm.

The client-side application:
This is the user interface. It will allow the users to interact with the application. The user can input data and request solutions.

The database:
The application data will be stored in the database. Those data include user data (email, password, ...) and project data (scenario based input data, solutions data, ...).

The waste collection optimization algorithm:
This piece of software was already developed by Bürgy et al. 20xx. It still needs to be considered when building the application since the application needs to provide input data to the algorithm and receive output data from it.

The server:
This is the core of the application. It interacts with the other three components by receiving data from the client-side application, by reading and writing to the database and by inputting and receiving data from the waste collection optimization algorithm.

[show inter-component architecture]

4.2. Server

After considering the screen mockup, the general architecture and the use case diagrams the server application has been structured the following way.

[insert class diagram]

At the core of the server is the data model. The model consists of several classes which have been designed to split the data into logical units. The classes that were developed are:

User.ts
Represents a user of the application. It is needed to manage logging in to the web application as well as to manage the access to the different projects (Project). Each user has an email, a password (which will be hashed by the Controller before it is stored) and a list of projects (Project) which specifies which projects the user can access. Additionally, it is specified if the user is an administrator or not (administrators have access to all projects).

MapNode.ts
Represents a node in a map. Each node has an id (which is unique), a x-coordinate, a y-coordinate and a population size. Additionally, it is specified on each node if it can serve as a vehicle depot of a vehicle (VehicleType) in a result (Result) and if it can serve as a waste depot in a collection point scenario (CollectionPointScenarioVersion).

MapArc.ts
Represents an arc in a map. It connects two nodes (MapNode), a source node and a destination node, and it stores the distance between those two nodes.

Graph.ts
Represents a map of a municipality. It consists of a list of nodes (MapNode) and a list of arcs (MapArc).

GarbageScenarioVersion.ts
Represents a concrete garbage scenario. It contains a timing and an estimation of waste on each node of the graph.

GarbageScenario.ts
Represents a garbage scenario with all its concrete garbage scenarios (GarbageScenarioVersion). It contains the title of the garbage scenario and a list of concrete garbage scenarios (GarbageScenarioVersion). Every time a garbage scenario is modified (i.e. the waste estimation of a node (MapNode) is changed) a new concrete garbage scenario (GarbageScenarioVersion) will be created and added to the list of concrete garbage scenarios.

CollectionPointScenarioVersion.ts
Represents a concrete collection point scenario. It contains a timing and it specifies on each node if it is a potential collection point or not.

CollectionPointScenario.ts
Represents a collection point scenario with all its concrete collection point scenarios (CollectionPointScenarioVersion). It contains the title of the collection point scenario and a list of concrete collection point scenarios (CollectionPointScenarioVersion). Every time a collection point scenario is modified (i.e. the specificatio if a node (MapNode) is a potential collection point or not is changed) a new concrete collection point scenario (CollectionPointScenarioVersion) will be created and added to the list of concrete collection point scenarios.

VehicleTypeVersion.ts
Represents a concrete vehicle type. It contains a timing, an average speed for a tour, an average speed when going to the depot, an average stop time and a vehicle capacity. Additionally, it specifies on each arc if the vehicle type can drive on it or not.

VehicleType.ts
Represents a vehicle type with all its concrete vehicle types (VehicleTypeVersion). It contains the title of the vehicle type and a list of concrete vehicle types (VehicleTypeVersion). Every time a vehicle type is modified (i.e. the specification if the vehicle type can drive on an arc (MapArc) is changed) a new concrete vehicle type (VehicleTypeVersion) will be created and added to the list of concrete vehicle types.

Tour.ts
Represents a tour calculated by the waste collection optimization algorithm following a solution request from the user through the web app. It belongs to a result (Result). It contains a timing, the time it takes to accomplish the tour, the estimated total waste collected during the tour, the tour nodes with their order and with their estimated collected waste and the concrete vehicle type (VehicleTypeVersion) that performs the tour.

Facility.ts
Represents the location of a facility on the map calculated by the waste collection optimization algorithm following a solution request from the user through the web app. It belongs to a result (Result). It contains a node (MapNode) and a waste capacity.

Result.ts
Represents the requested solution from the user through the web app. The input data from the user and the output data from the waste collection optimization algorithm are stored here. It contains a timing, a concrete garbage scenario (GarbageScenarioVersion), a concrete collection point scenario (CollectionPointScenarioVersion), a list of concrete vehicle types (VehicleTypeVersion) with its possible waste depot nodes (MapNode), the model (i.e. 'k1', 'k2', 'k3'), the maximal walking distance for a citizen to deposit his/her garbage, the minimal waste capacity of a facility (only for model 'k2'), the total cost of the solution, a list of tours (Tour), a list of facilities (Facility) and a boolean value specifying if the solution has been already calculated by the waste collection optimization algorithm. When a solution gets requested by a user, a result will get created using the public static async createResult method which also writes an input XML-file with the data given in by the user. The input XML-file is left in a folder and the waste collection optimization program then gets executed with the file provided as argument. The waste collection optimization program will leave an output XML-file in another folder as soon as it has calculated the results. The output XML-file will then be read and added to the result using its public setResultData method by the controller.

Project.ts
Represents an entire project. It contains the project title, a list of users that can access the project (User), a map (Graph), a list of garbage scenarios (GarbageScenario), a list of collection point scenarios (CollectionPointScenario), a list of vehicle types (VehicleType) and a list of results (Result). When creating a new project two SQL files are written, one for setting up the new project schema in the database (which is immediately executed) and the other for deleting it from the database (in case it will be necessary in the future).
//corrections are possible for the implementation of the software (and also, obviously, for the written master thesis)

Model.ts
The Model class represents all projects and all users of the application and was developed using the Singleton software development pattern, i.e. it assures that there is exactly one model instance which can be retrieved using its public static createModel() method. It contains a list of all users and all projects and, when created, also sets up the projects-users connections (by adding them to the user or project lists inside the projects respectively users instances). It handles the adding and deleting of projects and users from the application.

Each of these classes generally possess a (static) function to create instances of them and write the corresponding data to the database. Except for the Model class these classes also contain a (static) function which, when invoked, reads the database and returns the respective instances of that class. Additionally, these classes contain various getters and setters as well as adders and deleters (i.e. functions that add elements to an array variable or delete them from the array variable). If an object is part of another object, a reference is stored to the latter object. E.g. an arc (MapArc) must be part of a map (Graph), therefore the arc (MapArc) stores a reference to that map (Graph).

Additionally, some helper classes were developed. These include:

Logger
Why developping a Logger?
When any kind of applications run, errors can happen and in that case the programmer will need to debug the application. In order to debug, he needs information about the state of the application at the point of failure and how the application reached that state. Only then is the developer able to fix the application.
Logging is the writing of diagnostic information to protocols (source: IBM winston tutorial) which can be very helpful when debugging because it provides information on how the program reached the state of failure to the developer (source: IBM winston tutorial).
There are some libraries in Node.js (e.g. Winston or Log4js) that allow to manage logging in a simple way. Technically, logging could be done by simply using plain JavaScript but logging libraries provide some functionalities that would be time-consuming to code from ground-up (e.g. severity levels of messages).

Winston logging library
The winston logging library aspires to be a simple but still universal and flexible logging library. It supports multiple log channels and message levels. (source: https://github.com/winstonjs/winston)
Every winston logger instance can be configured to have several log channels. A log channel (in winston terminology called 'transport') is the destination of the log which can e.g. be the console or a file. For each log channel a message level ('silly', 'debug', 'verbose', 'info', 'warn' or 'error') can be specified. The specified message levels of the log channel and of the log message determine if an information will be logged or not. E.g. if the message level set on a log channel is 'info', a message will be logged if and only if it has the message level 'info' or higher (i.e. 'info', 'warn' or 'error') (source: IBM winston tutorial).
Extensive information on the winston logging library can be found on its official GitHub site https://github.com/winstonjs/winston.

Implementation
The Logger class handles the logging of the application and was developed using the Singleton software development pattern, i.e. it assures that there is exactly one logger instance which can be retrieved using its public static getLogger() method. The logger instance contains two winston logger instances. One of these winston logger instances writes database query informations while the other is responsible for all other loggings. The winston logger instance for database query information has one log channel (which is a file log channel) and its message level is set to 'silly' (i.e. all messages will be logged). The winston logger instance for non-database query information has three log channels two of which are file log channels with the message level set to 'silly' and 'info' respectively and one of which is a console log channel with the message level set to 'info'.
The programmer has to choose between two instance methods of the logger when he wants to log a message which are the public dbLog() and public fileAndConsoleLog() methods. For both of these methods the developer has to pass two parameters to the function, the information he wants to log and its severity (i.e. the message level). The dbLog() method should be used if the log message comes from an interaction between the server and the database. Otherwise the fileAndConsoleLog() method should be used.
The reason the logger instance contains two winston loggers was to ensure that the log files are splitted by information source. When debugging, this allows to independently analyze the database interactions and the other program errors/informations without each of them polluting each others log files. The reason the winston logger for non-database query informations has three log channels is that it ensures that there are log files with all non-database query logs and log files only with logs of higher severity levels. The log files with the logs with higher message levels allow to more quickly find error logs when debugging (since these log files are not polluted with logs of low message levels). Finally, the winston logger for non-database query informations has a console log channel in order to immediately notify the programmer that the application encountered a problem.
The winston loggers are updated every day before the first log is performed. Concretely, the filenames of the file log channels will be updated and named after todays date. Therefore the logs from different days will be written in different log-files which further allows the developer to more quickly find relevant logs.
(Check if needed to cite some things. See e.g. IBM winston logger tutorial)

Database handler
Why developping a Database handler?
There are two reasons a Database handler class was created. The first reason is to allow to easily query the database without needing to think about it's usual intricacies (e.g. creating a Pool, providing connection information, etc.) just by using one querying() method. The second reason is to allow the creation and deletion of projects which are complex tasks as it involves the creation, execution and deletion of sql-files.

node-postgres
Application data will be stored in a PostgreSQL database (see chapter xxx). The interfacing between the server and the database is done using the node-postgres modules which allow to interact with PostgreSQL databases (source: https://node-postgres.com/).
In order to connect to the PostgreSQL database either a connection Client or Pool (which contains a reusable list of connection clients) has to be created. There are some advantages in using a connection Pool over a Client. One reason is that the use of connection Pools comes with a performance increase. The reason is that a connection Client executes one query at a time when connected to a PostgreSQL database which would lead to a lower performance compared to the connection Pool which manages the execution of several queries among its list of connection Clients. Another reason to use a connection Pool over a Client is that the handshake that is performed when connecting a Client to the PostgreSQL database is very time-consuming (20-30 milliseconds). These costs can be minimized since a connection Pool manages a (reusable) list of connection Clients. (https://node-postgres.com/features/pooling)
The creation of a connection Pool (or a Client) requires connection information of the PostgreSQL database. This connection information includes the database user, password, host, port and name. This information can be provided in different ways e.g. by providing a connection string in the form 'postgresql://USER:PASSWORD@HOST:PORT/DB_NAME'. (https://node-postgres.com/features/connecting)
Database queries can then be executed by calling the query() instance method on the Pool and passing the query string as an argument. (https://node-postgres.com/features/pooling) (yes the 'pooling' site (under 'single query') and not the 'queries' site)
Extensive information on the node-postgres node.js modules can be found on its official Documentation site https://node-postgres.com/.

Implementation
The DatabaseHandler class handles the interactions with the PostgreSQL database and was developed using the Singleton software development pattern, i.e. it assures that there is exactly one DatabaseHandler instance which can be retrieved using its public static getDatabaseHandler() method. The public static getDatabaseHandler() sets the connection Pool as well as the users and projects schema (including its tables) in the database up, if necessary.
The DatabaseHandler contains a public async querying() instance method which, after being called with a query string as argument, executes a database query. Additionally, the class contains the public async setupProject() and public async deleteProject() instance methods that, after being called with the project title string as argument, setup or delete a project from the database, respectively. The setup of a project involves the creation of two sql-files, one for setting up and one for deleting the database schema of the project and its tables. The setup sql-file will be executed after its creation (in order to setup the database schema and its tables). The deletion sql-file will be executed when (and if) the project is deleted using the public async deleteProject() instance method. The public async deleteProject() instance method will also delete both sql-files that were created by the public async setupProject() instance method.

Controller
Why developping a Controller?
In order to have a properly functioning waste collection application server there are two needs that must be satisfied. Firstly, the server needs an interface in order to allow the client-side application to interact with it. Secondly, when the server receives information from the client-side application (and also the waste collection optimization algorithm) through its interface, it needs to update the waste collection application model based on the information it receives. Those two needs are satisfied by developing a Controller.

Technologies used
In order for the client-side application to interact with the server a REST web API using the Koa web framework was developed. The data is delivered to the client in JavaScript Object Notation (JSON) format.
REST API
An API, which is an acronym for application programming interface, is a set of rules which determine how different programs can communicate with each other (source: https://www.ibm.com/cloud/learn/api). An API allows that an application accesses a resource from another application. The application requesting a resource is called client while the application containing a resource is called server (source: https://www.ibm.com/cloud/learn/rest-apis). Todays most common APIs are web APIs which allow access to resources over the internet (source: https://www.ibm.com/cloud/learn/api).
An API functions by a client sending a request to the API through its URI (Uniform Resource Identifier). The API then calls the server after receiving the request. The server then returns a response to the API following the request of the client. The API then sends the data it received from the server to the client (source: https://www.ibm.com/cloud/learn/api).

[insert graph explaining functioning of API]

REST (Representational State Transfer) which was first proposed by Roy Fielding in his doctoral thesis is an architectural style for distributed hypermedia systems.(Fussnote 1) Roy Fielding derived REST by imposing six architectural constraints from a few network-based architectural styles and thus described it as a hybrid style. The six architectural constraints are:
Client-Server Separation
Leads to a separation of concerns. The user interface and the data storage are separated from each other.
Statelessness
Enforces that each client request contains all the necessary information to process it. There cannot be any stored context on the server.
Cache
Response data must be labeled as cacheable or not. Cache is data that can be reused by the client for future, equivalent requests and can thus reduce the client-server interactions.
Uniform interface
Data is exchanged through a standardized interface.
Layered System
REST allows architectural components to be structured hierarchically where each component exclusively knows the component that it is interacting with.
Code-on-demand
This is the only optional constraint. It allows clients to download executable code and thus extend its functionalities.
(cite Roy Fieldings properly)

(Fussnote 1): Hypermedia was suggested by Denise Tolhurst to be defined as "...any computer-based system that allows the interactive linking, and hence nonlinear traversal, of information that is presented in multiple forms that include text, still or animated graphics, movie segments, sounds, and music..." (add source! literal quote!!! Denise Tolhurst). Hypertext, contrary to hypermedia, refers only to static information which can include e.g. text, pictures and tables but e.g. not audio or video (source: Denise Tolhurst). The World Wide Web which was originally described as hypertext by Tim Berners-Lee (source: World-wide web: the information universe) is an example of hypermedia since it also presents non-static information such as audio and video.

While Roy Fielding developed REST from a theoretical perspective, his work gives little insight on how to practically implement a concrete REST Web Service. According to Alex Rodriguez nowadays a concretely implemented REST Web Service follows four principles. These are:
Statelessness
As already specified by Fielding, each request contains all necessary information (cite Fielding here).
In order to achieve a stateless service, the client is entirely responsible for storing any session state. When the client makes a request, it needs to include in the HTTP headers and body every piece of data that the server needs in order to respond i.e. the client shall make few assumptions about the server adding any context to the request. (cite Rodriguez here)
Use HTTP methods
The HTTP methods POST, GET, PUT and DELETE correspond to the CRUD operations (create, read, update, delete). Each HTTP method should only be used to execute the CRUD operation it is mapped to, i.e. a POST request creates a resource, a GET request reads a resource, a PUT request updates a resource and a DELETE request deletes a resource. (cite Rodriguez here)
Directory structure-like URIs
The design of REST Web Service URIs should be easy to understand (or even self-explanatory). This is achievable by defining directory structure-like URIs. E.g. the URI "http://www.myservice.org/discussion/topics/{topic}" exposes discussions about the topic "{topic}". On the other hand, if we wanted to expose the discussions in a certain language we would expose the URI "http://www.myservice.org/discussion/languages/{language}". (cite Rodriguez here)
Send JSON or XML (or both) requests/responses
JSON (JavaScript Object Notation) and XML (Extensible Markup Language) formats allow to present data objects in a simple and human-readable form. Resources should be transfered in one of these two data interchange formats. (cite Rodriguez here)

JSON
JSON is a data interchange format. JSON does not depend on any programming language but uses concepts that are relatable to programmers of most programming languages. It is based on two structures which are unordered sets of key-value pairs (also called objects) and ordered lists of elements. Equivalent structures to unordered sets of key-value pairs exist in most programming languages and are called e.g. dictionaries or hash tables while equivalent structures to ordered lists of elements are called e.g. arrays or lists. Each value of a key-value pair in an object can be an object, an ordered list, a string, a number or one of the the values "true", "false" or "null".
JSON is an ideal data interchange format since the structures that it is based on exist at least in some form in most programming languages. (cite https://www.json.org/json-en.html)
[insert example of JSON]

XML (Extensible Markup Language)
XML is a data format that allows to represent structured information (https://www.w3.org/standards/xml/core). XML is, as its name says, a markup language i.e. you can mark up content with tags. Tags are recognizable by the opening and closing angle brackets ('<' and '>'). In order to organize data in XML, the data needs to be placed between a beginning and ending tag. Such structures are called elements. A concrete element could be:
<date>21 January 1995</date>
Nesting of elements is also possible and each element can additionally contain attributes which are specified inside a tag:
<date calendar="gregorian">
    <day>21</day>
    <month>1</month>
    <year>1995</year>
</date>
Important to note is that a concrete XML-file will have exactly one root element.
The following piece of code shows the content of a concrete XML-file (including an optional XML declaration on the first line):
[include XML-file content]

Koa
Koa is a web framework for nodejs that allows to create web APIs.
A Koa object contains an array of middleware functions which will be executed upon request. The middleware functions are specified by passing them as parameters to the instance method use() on a koa object. A specified middleware passes control downstream to the next middleware by invoking the async next() function. When there are no more middlewares to execute the control flow will go back upstream. (source: https://koajs.com/)
[insert easy koa middleware example here]

There are several nodejs modules that can be used with the Koa web framework. These modules include:
koa-router
Allows to configure routing which means that we can specify how the API will respond to a request to a certain endpoint (which is a HTTP method and a URI) (source: https://expressjs.com/en/starter/basic-routing.html). Thus a router middleware function will only be executed if the request is targeted at its specified endpoint. We can specify a route on a koa-router by invoking its instance methods use(), get(), post(), del() or patch(). While the use() method will specify a route that can be matched regardless of the requests HTTP method, routes specified with the other methods (i.e. get(), post(), del() and patch()) can only be matched if the respective HTTP method was used for the request. We then have to provide to these methods a URI as parameter as well as a function that specifies how the request is handled. The specified route can only be matched if the requests URI matches with the beginning of the URI we specified on the route. The specified routes on the koa-router are then added as middleware to the koa object by invoking the use() instance method on the koa object and passing to it the routes middleware functions (which are retrieved with the instance method routes() on the koa-router) as parameters.
[insert good example for koa-router]
Additionally, the koa-routers allowedMethods() instance method returns a middleware that can handle OPTIONS requests.

koa2-cors
What is CORS?
CORS stands for Cross-Origin Resource Sharing. It is a mechanism based on certain HTTP-headers that permits a server to specify which origins (i.e. a combination of scheme, domain and port) other than its own can access its resources from a browser. In its simplest use, CORS is handled by setting two HTTP-headers, one request header (which is the Origin header) and one response header (which is the Access-Control-Allow-Origin header). The Origin header will always, as its name says, show the origin of the request. The Access-Control-Allow-Origin header will show which origins are allowed to access the resources of the server. If the Origin and Access-Control-Allow-Origin do not match the request will fail. There is also the possibility to send so called preflighted requests which allow to determine if the actual request can be sent without encountering a CORS failure. (source: https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS)
Additionally, CORS can handle credentialed requests (i.e. with HTTP cookies) by setting the Access-Control-Allow-Credentials response header to true. If the Access-Control-Allow-Credentials response header is not set to true the browser will reject the response. (source: https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS)
The koa2-cors module provides CORS middleware for Koa. It allows to set several CORS headers.

Additional modules used in the development of the Controller are the koa-bodyparser which provides us a middleware that parses the body that was sent through the HTTP request (so it is accessible in the Koa app through ctx.request.body) and the koa-logger which logs information about requests and responses to the Koa app (thus can be useful for debugging the application).

Chokidar
Chokidar is a file watching library (source: https://www.npmjs.com/package/chokidar). It concretely allows e.g. to specify what a program should do when a file is added to a folder.

bcrypt
A library to generate and check hashed passwords. (https://www.npmjs.com/package/bcrypt)

JSON Web Token (JWT)
JWT is an open standard that allows the secure transmission of data between parties. JSON Web Tokens can be digitally signed using a secret key with the HMAC (Hash-based Message Authentication Code) algorithm. JWTs are most commonly used for handling Authorization. When the user logs into an application a JWT can be returned which then has to be sent by the client to the server every time a resource is accessed. The server then needs to authenticate the user by verifying the JWT and check if the user is authorized to access that resource. (source: https://jwt.io/introduction/)

jsonwebtoken
This nodeJS module implements JSON Web Tokens.

Implementation
The Controller class handles the interactions with the server and was developed using the Singleton software development pattern, i.e. it assures that there is exactly one Controller instance which can be retrieved using its public static createController() method. The public static createController() method stores a reference to the data model (i.e. all projects and users) in a variable and sets up the Koa object by adding the middlewares to it. Additionally, a file watch is set up which handles the file outputs of the waste collection optimization program.
The middlewares added to the koa object include the koa-logger, koa-bodyparser, koa2-cors and the koa-routers routes that were set up. A middleware that responds to OPTIONS requests was also added.
The routing middleware functions that were set up can be splitted in two groups. In the first group we have middleware functions that have generic preparatory purposes while in the second group we have specific handlers for our endpoints. The difference between these two types of functions can be easily understood with an example. When a GET request to the URI '/api/protected/project/fribourg' is made the Controller needs to first authenticate the user, then check if the user is allowed to access the project 'fribourg' and then send the data of the project 'fribourg' back. When a DELETE request to the URI '/api/protected/project/bern' is made the Controller needs to first authenticate the user, then check if the user is allowed to access the project 'bern' and then delete the project 'bern'. We see a pattern here. The first two steps of the GET and the DELETE request are almost the same (only the projectname being different). The first step needs to be executed every time a request is made to a URI starting with '/api/protected' while the second step needs to be executed every time a request is made to a URI starting with '/api/protected/project/{projectname}'. This means that we can set up generic preparatory middleware functions that are executed when a request is made to a URI that starts with those strings. On the other hand, the specific handlers of our endpoints is then the middleware function that handles (in our example) the retrieving of the project 'fribourg' data and the deletion of the project 'bern'.

The specified generic preparatory middleware functions are:
/api/protected
Reads the JSON Web Token of the request (which is sent as a cookie) and verifies it in order to authenticate the user.
/api/protected/project/:projectname
Most importantly, finds the Project object with the provided projectname in the URI and checks if the user is allowed to access that project.
/api/protected/project/:projectname/user/:email
Most importantly, finds the user object with the provided email in the URI and checks if the user, who makes the request, is allowed to add or delete that user to respectively from the project.
/api/protected/user/:email
Most importantly, finds the user object with the provided email in the URI and checks if the user, who makes the request, is allowed to access that user.

The following handlers of our endpoints accept in some cases request bodies. The schema of the request bodies is visualized by using the TypeScript type declaration of objects. E.g. the following body has a property 'title' of type string and a property 'count' of type number:
{ title: string; count: number; }

The specified specific handlers of our endpoints are:
POST /api/public/register
Request body: { email: string; admin: boolean; password: string; }
The new user gets created using the users data from the request body. The password is hashed before creating the user.
POST /api/public/login
Request body: { email: string; password: string; }
The user object is found and the password from the request body is then compared with the stored hashed password. If the request bodys password is correct, a JSON Web Token gets signed and sent to the client as a cookie.
GET /api/public/logout
No request body necessary.
User gets logged out by setting the cookie holding the JSON Web Token to an empty string which expires immediately.
POST /api/protected/newproject/:projectname
Request body: { xml: string; }
Creates a new project with the projectname provided in the URI. The map (graph) of the municipality gets set using the xml data in the request body. This endpoint is only accessible to administrators.
DELETE /api/protected/project/:projectname
No request body necessary.
Deletes the project with the projectname provided in the URI. This endpoint is only accessible to administrators.
PUT /api/protected/project/:projectname
Request body: { newProjectname: string; }
Finds the project with the projectname provided in the URI. Then updates its projectname using the newProjectname value from the request body.
GET /api/protected/project/:projectname
No request body necessary.
Finds the project with the projectname provided in the URI. Then returns its data as a JSON object which includes nodes, arcs, garbage scenarios, collection point scenarios, vehicle types, results and users data.
GET /api/protected/projects
No request body necessary.
Returns data that describes which user(s) can modify which project(s). This endpoint is only accessible to administrators.
DELETE /api/protected/user/:email
No request body necessary.
Deletes the user with the email provided in the URI.
PUT /api/protected/user/:email
Request body: { newEmail: string; newAdmin: boolean; newPassword: string; }
Finds the user object with the email provided in the URI and updates its data to the values provided in the request body. The new password gets hashed before the update.
GET /api/protected/user/:email
No request body necessary.
Finds the user with the email provided in the URI. Then returns its data as a JSON object which includes its email, admin status (true/false), (hashed) password and the projects the user is allowed to access.
GET /api/protected/users
No request body necessary.
Returns data that describes which user(s) can modify which project(s). This endpoint is only accessible to administrators.
POST /api/protected/project/:projectname/user/:email
No request body necessary.
Adds the user with the email provided in the URI to the project with the projectname provided in the URI (i.e. the user gets the rights to access the project).
DELETE /api/protected/project/:projectname/user/:email
No request body necessary.
Deletes the user with the email provided in the URI from the project with the projectname provided in the URI (i.e. the user loses the rights to access the project).
POST /api/protected/project/:projectname/garbageScenario
Request body: { title: string; nodesWaste: { nodeid: number; wasteEstimation: number }[] }
Creates a new garbage scenario with the title provided by the request body and then adds a new garbage scenario version to it using the nodes waste data from the request body.
DELETE /api/protected/project/:projectname/garbageScenario/:title
No request body necessary.
Deletes the garbage scenario with the title in the URI from the project with the projectname in the URI.
PUT /api/protected/project/:projectname/garbageScenario
Request body: { title: string; nodesWaste: { nodeid: number; wasteEstimation: number }[]; newTitle: string; }
Updates the title of the garbage scenario with the new title provided in the request body and adds a new garbage scenario version to it using the nodes waste data from the request body.
POST /api/protected/project/:projectname/collectionPointScenario
Request body: { title: string; nodesPotCP: { nodeid: number; potentialCollectionPoint: boolean }[] }
Creates a new collection point scenario with the title provided by the request body and then adds a new collection point scenario version to it using the nodes potential collection points data from the request body.
DELETE /api/protected/project/:projectname/collectionPointScenario/:title
No request body necessary.
Deletes the collection point scenario with the title in the URI from the project with the projectname in the URI.
PUT /api/protected/project/:projectname/collectionPointScenario
Request body: { title: string; nodesPotCP: { nodeid: number; potentialCollectionPoint: boolean }[]; newTitle: string; }
Updates the title of the collection point scenario with the new title provided in the request body and adds a new collection point scenario version to it using the nodes potential collection points data from the request body.
POST /api/protected/project/:projectname/vehicleType
Request body: { title: string; averageSpeedTour: number; averageSpeedDepot: number; averageStopTime: number; vehicleCapacity: number; arcsActivated: { sourceNodeID: number; destinationNodeID: number; activated: boolean }[] }
Creates a new vehicle type with the title provided by the request body and then adds a new vehicle type version to it using the activated arcs, average tour speed, average depot speed, average stop time and vehicle capacity data from the request body.
DELETE /api/protected/project/:projectname/vehicleType/:title
No request body necessary.
Deletes the vehicle type with the title in the URI from the project with the projectname in the URI.
PUT /api/protected/project/:projectname/vehicleType
Request body: { title: string; averageSpeedTour: number; averageSpeedDepot: number; averageStopTime: number; vehicleCapacity: number; arcsActivated: { sourceNodeID: number; destinationNodeID: number; activated: boolean }[]; newTitle: string; }
Updates the title of the vehicle type with the new title provided in the request body and adds a new vehicle type version to it using the activated arcs, average tour speed, average depot speed, average stop time and vehicle capacity data from the request body.
POST /api/protected/project/:projectname/result
Request body: { garbageScenarioTitle: string; garbageScenarioTiming: string; collectionPointScenarioTitle: string; collectionPointScenarioTiming: string; vehTypeVersAndWasteDepotNodes: { vehicleTypeTitle: string; vehicleTypeTiming: string; availableWasteDepotNodes: { nodeid: number }[]; }[]; model: string; maxWalkingDistance: number; minWaste: number; }
Adds a result to the project with the projectname provided in the URI using the request bodies data. This initiates the calculation of a concrete result by the waste collection optimization algorithm by writing an input XML file to a folder which then is accessed and read by the waste collection optimization program.
DELETE /api/protected/project/:projectname/result/:resulttiming
No request body necessary.
Deletes the result with the result timing provided in the URI from the project with the projectname provided in the URI.

As soon as the waste collection optimization program has calculated a result, it creates an XML output file which is left in a specified folder. The server then gets notified by a file watcher that an XML output file has been created and loads the files data into the program (and database).

4.3. Database
Why a Database?
The data that the user defines needs to be stored in order to be permanently available.

PostgreSQL
PostgreSQL was chosen as the database to store the applications data. It is an open-source object-relational database management system (source: https://www.postgresql.org/docs/current/intro-whatis.html). SQL (structured query language), which is a programming language that permits the querying and manipulation of data in a relational database (source: https://www.ibm.com/cloud/blog/sql-vs-nosql), is largely supported by PostgreSQL (source: https://www.postgresql.org/docs/current/intro-whatis.html).
There are a few hierarchical units that we have to be aware of when working with PostgreSQL. The top hierarchical unit of PostgreSQL is the database cluster which is managed by exactly one database server instance and contains (possibly) many databases (source: https://www.postgresql.org/docs/12/creating-cluster.html).
A database can then contain several schemas which can contain several tables. Schemas allow to separate database objects in groups in order to make them easier to manage (source: https://www.postgresql.org/docs/13/ddl-schemas.html). A table, on the other hand, is a unit where data is stored in a row-based manner where each row has the same named columns (source: https://www.postgresql.org/docs/9.1/tutorial-concepts.html).

Implementation
In the PostgreSQL database cluster used for this project one database was specified. This database was named 'wastecollectiondata'. The schemas contained in the wastecollectiondata database are:
-A 'usersprojects' schema: This schema contains the tables users, projects and userprojects. These tables store the users data (email, admin status and password), projects data (projectname) and the user-project connections (i.e. which users can access which projects) respectively.
[show database schema of usersprojects schema]
-Projects schemas: Each project has its own schema named after the projectnames in the usersprojects.projects table. It is generally corresponds to the server data model.
[show database schema of a project schema]

4.4. Client
Overview
The prototype that was created during the requirements engineering process was used as a blueprint for the development of the client-side application.
The frontend was developed as a Single-Page Application. A Single-Page Application is a web application in which every interaction with it happens on a one web page. In a Single-Page Application requests of small pieces of data to the server happen and are then used to modify the DOM. This is very different from a classical (multi-page) web application where after every interaction the browser loads a new web page from the server (source: Ali Mesbah page 10 Analysis and Testing of Ajax-based Single-page Web Applications).

Technologies
Vue.js
The client-side application of the project was built using the Vue.js (or simply Vue) framework.
From a technical perspective, VueJS implements the Model-View-ViewModel (MVVM) design pattern which has three components (View, ViewModel and Model). The Model in Vue.js is represented by plain JavaScript objects while the View is represented by the DOM. The ViewModel sits in between the View and the Model and is responsible for syncing the data between them. The ViewModel sets up the data bindings on the View and gets notified when the Models data changes, achieving reactivity of the View and the Model. (source: https://012.vuejs.org/guide/)
One of Vues most important concepts is the components system. It allows to define self-contained and reusable elements that can be nested in other elements respectively nest other elements in them (source: https://vuejs.org/v2/guide/#Composing-with-Components). Let's consider an arbitrary example: a travel blog. We have a root component which nests other components, e.g. a toolbar and the content. The content component has descriptive text in it and nests another component (which is a list of travel pictures). Each travel picture of the list is a component which is nested inside the list component. We see that this component system allows to hierarchically organize the frontend application.
A Vue component can hold several properties such as e.g. props (which are data that are passed from a parent component to a child component), data, methods and a template (source: https://v3.vuejs.org/guide/component-basics.html). The template property of a component has an HTML-based syntax that allows to declaratively render the component data to the DOM (foot note 1) (source: https://vuejs.org/v2/guide/syntax.html). A component can be defined in a file (referred to as Single File Component) which has the .vue extension. The .vue file can specify a template using the template tag and add logic to the component inside the script tag (source: https://vuejs.org/v2/guide/single-file-components.html).
The following two images provide an example single file component and how it is displayed. We can see how the data is declaratively rendered:
[definition of simple single file component]
[screenshot of display of previous code]

During the projects development other libraries and frameworks wer used that are compatible with Vue. These include:
Vuex
Vue-router
Vuetify

Vuex
Managing a common state across multiple Vue.js components can become tedious in more complex applications. The shared data has to be passed as props down the component hierarchy to the components which use that data. Then every time the data (or props) modification is triggered in a component, that component has to initiate an event chain which fires up the component hierarchy until it reaches the component that holds the shared data which can then finally be modified (source: https://vuex.vuejs.org/).
Vuex is a state management library for Vue web applications. It provides a centralized store for the Vue.js applications shared data. The advantage of managing shared data in Vue with Vuex instead of the traditional way is that it allows any component to access (i.e. read and modify) data from the centralized store directly through the methods (in Vuex called getters, mutations and actions) which the programmer defines (source: https://vuex.vuejs.org/).

Vue router


[Explain how components usually communicate with each other (i.e. through events) in order to make transition to Vuex]
[Explain Vue-router]
[Explain Vuetify]

Foot note 1: The DOM (Document Object Model) constitutes the representation of data objects which embody the structure and content of a web document (source: https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model/Introduction).

Implementation


4.5. Testing
What is Testing? see IBM
Testing (of software) is the process of verification that a software works the way it is supposed to work. It can prove useful in order to prevent bugs (source: https://www.ibm.com/topics/software-testing).
There are several types of testing. The types of testing that were used in this project are acceptance and integration testing. While integration testing verifies if several software components work correctly together, acceptance testing checks if the software system as a whole functions the way it is supposed to (source: https://www.ibm.com/topics/software-testing).
Technologies used
Jest 
Jest is a JavaScript testing framework (source: https://jestjs.io/). It allows to easily set up test suites using its API. Functionalities of Jest include setting up (named) tests (source: https://jestjs.io/docs/getting-started), checking values using matchers (source: https://jestjs.io/docs/using-matchers), testing asynchronous code (source: https://jestjs.io/docs/asynchronous), setup and teardown of tests (source: https://jestjs.io/docs/setup-teardown) and creation of test doubles (which is replacement code for some parts of the production code (source: https://developer.ibm.com/languages/node-js/tutorials/learn-nodejs-unit-testing-in-nodejs/)) (source: https://jestjs.io/docs/mock-functions).
Methodologies
The server as well as the client were tested during the development of the software. While the server was tested using mostly automated testing (with Jest) the client was tested manually. Test suites were written to test the different classes on the server. In these test suites not all but the 'critical' methods of the classes were tested. The critical methods are those that were judged more likely to fail. A method would be critical if it contains complex code (which means that the likelihood of the programmer implementing the code wrongly is higher) or code that connects with other software components (database, other classes). Simple getters and setters were not considered critical and thus not tested. Test doubles were developed for cases where the inclusion of production code was not practical and necessary.
While the testing of the server would be considered integration testing (i.e. testing if different components work correctly together) the client-side testing allowed to test the system as a whole (i.e. acceptance testing) (do I need to cite that again?) because the client makes requests to the server which in turn connects to the database (and the waste collection optimization program (maybe delete this because it was not tested)) thus involving the whole system. Testing on the client-side was done manually meaning that different test cases (e.g. logging in to the application, creating a new project, requesting solutions, etc.) were tested by clicking through the frontend application.

[components of the project]
